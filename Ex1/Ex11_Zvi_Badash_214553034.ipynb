{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 11 - Broadcasting\n",
    "### Zvi Badash 214553034"
   ],
   "metadata": {
    "id": "9N59pGcsb64L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As instructed, I recorded a video of myself explaining the exercise and the code I wrote for it.\n",
    "The video unfortunately has a watermark, but I hope it won't be in the way of understanding the code.\n",
    "\n",
    "[Exercise 11 - Broadcasting](https://youtu.be/KXYBFXhQpkE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "gkQWBZ58SNCX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Tensor = torch.Tensor\n",
    "Size = torch.Size"
   ],
   "metadata": {
    "id": "NlPAbdWLckK-"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "## Copied from chatGPT ##\n",
    "\n",
    "def print_with_color(color: int, msg: str) -> None:\n",
    "    \"\"\"\n",
    "    Prints a message with a given color.\n",
    "    :param color: 1 for red, 2 for green, 3 for yellow, 4 for blue, 5 for magenta, 6 for cyan, 7 for white\n",
    "    :param msg: the message to print\n",
    "    \"\"\"\n",
    "    print(f'\\033[38;5;{color}m{msg}\\033[0m')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question A - ```expand_as```"
   ],
   "metadata": {
    "id": "JkFzw90OcLkz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def expand_as(A: Tensor, B: Tensor, suppress_len_check: bool=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Expands tensor A to the size of tensor B.\n",
    "    :param A: The tensor to expand\n",
    "    :param B: The tensor to expand to\n",
    "    :param suppress_len_check: If True, the function will not check if A has more dimensions than B\n",
    "    :return: The expanded tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Define A's and B's sizes for later use\n",
    "    A_size: Size = A.shape\n",
    "    B_size: Size = B.shape\n",
    "\n",
    "    # Check if A can be expanded to B's size\n",
    "    ### Check if A has more dimensions than B\n",
    "    if not suppress_len_check and len(A_size) > len(B_size):\n",
    "        raise RuntimeError(f'The number of sizes provided for tensor B ({len(B_size)}) must be greater or equal to the number of dimensions in tensor A ({len(A_size)})')\n",
    "   \n",
    "    ### Check if some dimensions are incompatible\n",
    "    reversed_sizes_zip = zip(A_size[::-1], B_size[::-1])\n",
    "    for i, (a, b) in enumerate(reversed_sizes_zip): # Loop through the sizes in reverse\n",
    "        if (a != 1 and b != 1 and a != b) or a > b:\n",
    "            raise RuntimeError(f'The size of tensor A ({a}) must match the size of tensor B ({b}) at non-singleton dimension {i}')\n",
    "\n",
    "    # Clone A and unsqueeze it along missing A dimensions (Append 1 sized dimensions)\n",
    "    expanded_tensor = A.clone()\n",
    "    for _ in range(len(B_size) - len(A_size)):\n",
    "        expanded_tensor = torch.unsqueeze(expanded_tensor, 0)\n",
    "\n",
    "    # Find dimensions that needs repeating along\n",
    "    broadcast_along_dims = [i for i, (c, b) in enumerate(zip(expanded_tensor.shape, B_size)) if c == 1]\n",
    "\n",
    "    # Repeat the tensor along each dimension\n",
    "    for dim in broadcast_along_dims:\n",
    "        expanded_tensor = torch.cat([expanded_tensor] * B_size[dim], dim)\n",
    "    \n",
    "    # Return the expanded tensor\n",
    "    return expanded_tensor"
   ],
   "metadata": {
    "id": "B2x3v8SAcL4T"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question B - ```is_broadcastable```"
   ],
   "metadata": {
    "id": "MIrx3Tb7cuoU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def are_broadcastable(A: Tensor, B: Tensor) -> Tuple[bool, Optional[Size]]:\n",
    "    \"\"\"\n",
    "    Checks if tensors A and B can be broadcasted together.\n",
    "    :param A: The first tensor\n",
    "    :param B: The second tensor\n",
    "    :return: A tuple of a boolean and a torch.Size object.\n",
    "             The boolean indicates if the tensors can be broadcasted together.\n",
    "             The torch.Size object is the broadcasted size of the tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define A's and B's sizes for later use\n",
    "    A_size: List = list(A.shape)\n",
    "    B_size: List = list(B.shape)\n",
    "\n",
    "    # Check that A can be broadcasted to B's size\n",
    "    ### Check if some dimensions are incompatible\n",
    "    reversed_sizes_zip = list(zip(A_size[::-1], B_size[::-1]))\n",
    "    for i, (a, b) in enumerate(reversed_sizes_zip): # Loop through the sizes in reverse\n",
    "        if a != 1 and b != 1 and a != b:\n",
    "            return False, Size() # Return an empty size and False\n",
    "\n",
    "    # Determine the broadcasted tensor size\n",
    "    ### Insert 0s at the beginning of the shorter dimension size\n",
    "    if len(A_size) != len(B_size):\n",
    "        shorter = A_size if len(A_size) < len(B_size) else B_size\n",
    "        while len(A_size) != len(B_size):\n",
    "            shorter.insert(0, 0)\n",
    "\n",
    "    # At this point both tensors must have the same number of dimensions\n",
    "    # (Either because they had the same length to begin with or we appended\n",
    "    # enough 0s to the shorter tensor sizes)\n",
    "    assert len(A_size) == len(B_size)\n",
    "\n",
    "    # Return the broadcasted tensor size\n",
    "    return True, torch.Size([max(a, b) for a, b in zip(A_size, B_size)])"
   ],
   "metadata": {
    "id": "FQTVLzlFcu5-"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question C - ```broadcast_tensors```"
   ],
   "metadata": {
    "id": "2L0USuDwcvSD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def broadcast_tensors(A: Tensor, B: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Broadcasts tensors A and B together.\n",
    "    :param A: The first tensor\n",
    "    :param B: The second tensor\n",
    "    :return: The broadcasted tensors\n",
    "    \"\"\"\n",
    "    _, broadcasted_size = are_broadcastable(A, B) # Get the broadcasted size\n",
    "\n",
    "    # Expand A and B to the broadcasted size and return them\n",
    "    return expand_as(A, torch.empty(broadcasted_size), suppress_len_check=True), \\\n",
    "           expand_as(B, torch.empty(broadcasted_size), suppress_len_check=True)"
   ],
   "metadata": {
    "id": "U5FC0qrTcv17"
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question D - Testing"
   ],
   "metadata": {
    "id": "nTpGfXHFcwHb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In the following test I'm generating two random tensors (using `randn`) and I try to first expand them to each-others sizes and then broadcast them together\n",
    "\n",
    "I also check if operations between the broadcasted tensors is the same as between the original tensors (in that case the operation has been automatically \n",
    "lifted by `torch`).\n"
   ],
   "metadata": {
    "id": "EFr0MaspzDTA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create tests list\n",
    "As = []\n",
    "Bs = []"
   ],
   "metadata": {
    "id": "vYYaPszp2kJL"
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the first simple test, I chose two random tensors `A` and `B` which have the same shape.\n"
   ],
   "metadata": {
    "id": "IKn6j4pSy1ET"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# First test\n",
    "As.append(torch.randn(3, 2))\n",
    "Bs.append(torch.randn(3, 2))"
   ],
   "metadata": {
    "id": "KGFjjPDzzqwE"
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the second test, I chose two random tensors `A` and `B` which are broadcastable but `A` can't be expanded to `B`'s size (or vice versa).\n"
   ],
   "metadata": {
    "id": "2lVR5cgD_Po4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Second test\n",
    "#                        |     |\n",
    "#                        ⌄     ⌄\n",
    "As.append(torch.randn(1, 2, 4, 1))\n",
    "Bs.append(torch.randn(4, 1, 4, 6))"
   ],
   "metadata": {
    "id": "eXX6COWC6RfY"
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the third test, I chose two random tensors `A` and `B` with a different number of dimensions to make sure this capability works as well.\n"
   ],
   "metadata": {
    "id": "VhSUfANK0OUP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Third test\n",
    "#                              |     |\n",
    "#                              ⌄     ⌄\n",
    "As.append(torch.randn(         1, 7, 2))\n",
    "Bs.append(torch.randn(4, 1, 5, 6, 7, 1))"
   ],
   "metadata": {
    "id": "yp_LEosb0NQ_"
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the last test, I chose two random tensors `A` and `B` with a different number of dimensions, such that `A` can be expanded to `B`'s size but `B` can't be expanded to `A`'s size.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Fourth test\n",
    "As.append(torch.randn(         1, 7, 2))\n",
    "Bs.append(torch.randn(4, 1, 5, 6, 7, 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for A, B in zip(As, Bs):\n",
    "    print(f'*** Checking tensors of shape {A.shape} and {B.shape} ***')\n",
    "\n",
    "    # Check if the tensors can be expanded to each-others sizes\n",
    "    try:\n",
    "        if torch.all(A.expand_as(B) == expand_as(A, B)).item():\n",
    "            print_with_color(2, f'\\t+ Expansion from tensor of shape {A.shape} to shape {B.shape} succeeded')\n",
    "    except RuntimeError:\n",
    "        print_with_color(1, f'\\t- Expansion between tensor of shape {A.shape} to shape {B.shape} failed')\n",
    "\n",
    "    try:\n",
    "        if torch.all(B.expand_as(A) == expand_as(B, A)).item():\n",
    "            print_with_color(2, f'\\t+ Expansion from tensor of shape {B.shape} to shape {A.shape} succeeded')\n",
    "    except RuntimeError:\n",
    "        print_with_color(1, f'\\t- Expansion between tensor of shape {B.shape} to shape {A.shape} failed')\n",
    "\n",
    "    # Check if the broadcasted tensors are of correct size\n",
    "    broadcastable, size = are_broadcastable(A, B)\n",
    "    if broadcastable:\n",
    "        assert (A + B).shape == size\n",
    "    else:\n",
    "        print_with_color(1, f'\\t- Tensors of shapes {A.shape} and {B.shape} are non-compatible for broadcasting.')\n",
    "\n",
    "    # Check that the broadcasting itself is correct\n",
    "    if broadcastable:\n",
    "        C, D = broadcast_tensors(A, B)\n",
    "        assert torch.all(A + B == C + D).item()\n",
    "        assert torch.all(A * B == C * D).item()\n",
    "        assert torch.all(A / B == C / D).item()\n",
    "        print_with_color(2, f'\\t+ Broadcasting between tensors of shapes {A.shape} and {B.shape} succeeded')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2be2209c-afd4-4ce9-bf9b-73bce809c14a",
    "id": "5GF7MDj40NQ_"
   },
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Checking tensors of shape torch.Size([3, 2]) and torch.Size([3, 2]) ***\n",
      "\u001B[38;5;2m\t+ Expansion from tensor of shape torch.Size([3, 2]) to shape torch.Size([3, 2]) succeeded\u001B[0m\n",
      "\u001B[38;5;2m\t+ Expansion from tensor of shape torch.Size([3, 2]) to shape torch.Size([3, 2]) succeeded\u001B[0m\n",
      "\u001B[38;5;2m\t+ Broadcasting between tensors of shapes torch.Size([3, 2]) and torch.Size([3, 2]) succeeded\u001B[0m\n",
      "*** Checking tensors of shape torch.Size([1, 2, 4, 1]) and torch.Size([4, 1, 4, 6]) ***\n",
      "\u001B[38;5;1m\t- Expansion between tensor of shape torch.Size([1, 2, 4, 1]) to shape torch.Size([4, 1, 4, 6]) failed\u001B[0m\n",
      "\u001B[38;5;1m\t- Expansion between tensor of shape torch.Size([4, 1, 4, 6]) to shape torch.Size([1, 2, 4, 1]) failed\u001B[0m\n",
      "\u001B[38;5;2m\t+ Broadcasting between tensors of shapes torch.Size([1, 2, 4, 1]) and torch.Size([4, 1, 4, 6]) succeeded\u001B[0m\n",
      "*** Checking tensors of shape torch.Size([1, 7, 2]) and torch.Size([4, 1, 5, 6, 7, 1]) ***\n",
      "\u001B[38;5;1m\t- Expansion between tensor of shape torch.Size([1, 7, 2]) to shape torch.Size([4, 1, 5, 6, 7, 1]) failed\u001B[0m\n",
      "\u001B[38;5;1m\t- Expansion between tensor of shape torch.Size([4, 1, 5, 6, 7, 1]) to shape torch.Size([1, 7, 2]) failed\u001B[0m\n",
      "\u001B[38;5;2m\t+ Broadcasting between tensors of shapes torch.Size([1, 7, 2]) and torch.Size([4, 1, 5, 6, 7, 1]) succeeded\u001B[0m\n",
      "*** Checking tensors of shape torch.Size([1, 7, 2]) and torch.Size([4, 1, 5, 6, 7, 2]) ***\n",
      "\u001B[38;5;2m\t+ Expansion from tensor of shape torch.Size([1, 7, 2]) to shape torch.Size([4, 1, 5, 6, 7, 2]) succeeded\u001B[0m\n",
      "\u001B[38;5;1m\t- Expansion between tensor of shape torch.Size([4, 1, 5, 6, 7, 2]) to shape torch.Size([1, 7, 2]) failed\u001B[0m\n",
      "\u001B[38;5;2m\t+ Broadcasting between tensors of shapes torch.Size([1, 7, 2]) and torch.Size([4, 1, 5, 6, 7, 2]) succeeded\u001B[0m\n"
     ]
    }
   ]
  }
 ]
}
