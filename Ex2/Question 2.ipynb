{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Zvi Badash 214553034\n",
    "### Question 2 - Demo Autograd Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As instructed, I recorded a video of myself explaining the exercise and the code I wrote for it.\n",
    "The video unfortunately has a watermark, but I hope it won't be in the way of understanding the code.\n",
    "\n",
    "[Exercise 12 - My Scalar](https://youtu.be/VHfUC44Y9ek)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and type aliases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "Size = torch.Size\n",
    "Tensor = torch.Tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The MyScalar class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MyScalar:\n",
    "    \"\"\"\n",
    "    A scalar class that supports autograd.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: float, imm_grad=None, parent=None):\n",
    "        \"\"\"\n",
    "        :param value: The value of the scalar\n",
    "        :param imm_grad: The immediate gradient of the scalar w.r.t its parent\n",
    "        :param parent: The parent of the scalar (The scalar that this scalar is a function of)\n",
    "        \"\"\"\n",
    "        self.value: float = value\n",
    "        self.imm_grad: float | None = imm_grad\n",
    "        self.parent: MyScalar | None = parent\n",
    "\n",
    "    def _get_gradient(self, grad_dict: dict[int, float]):\n",
    "        \"\"\"\n",
    "        This method is the actual recursive method that computes the gradients of this scalar w.r.t all of its parents.\n",
    "        The calculation is done using the chain rule for single-variable functions.\n",
    "        :param grad_dict: The current dictionary of gradients\n",
    "        :return: A dictionary of the gradients of this scalar w.r.t all of its parents.\n",
    "        \"\"\"\n",
    "        if self.parent is None:\n",
    "            return grad_dict\n",
    "        else:\n",
    "            grad_dict[id(self.parent)] = grad_dict[id(self)] * self.imm_grad\n",
    "            return self.parent._get_gradient(grad_dict)\n",
    "\n",
    "\n",
    "    def get_gradient(self):\n",
    "        \"\"\"\n",
    "        This method returns a dictionary of the gradients of this scalar w.r.t all of its parents.\n",
    "        The keys of the dictionary are the ids of the parents, and the values are the gradients.\n",
    "        I chose to use ids instead of the actual parents because it's less memory intensive in my opinion.\n",
    "        This method is merely a wrapper for the recursive method _get_gradient.\n",
    "        :return: A dictionary of the gradients of this scalar w.r.t all of its parents\n",
    "        \"\"\"\n",
    "        return self._get_gradient({id(self): 1.})\n",
    "\n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        :return: A new MyScalar object with the value of the log of this scalar\n",
    "        \"\"\"\n",
    "        return MyScalar(\n",
    "            torch.log(torch.tensor(self.value)).item(),\n",
    "            1. / self.value,\n",
    "            self\n",
    "        )\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        :return: A new MyScalar object with the value of the exponent of this scalar\n",
    "        \"\"\"\n",
    "        return MyScalar(\n",
    "            torch.exp(torch.tensor(self.value)).item(),\n",
    "            torch.exp(torch.tensor(self.value)).item(),\n",
    "            self\n",
    "        )\n",
    "\n",
    "    def sin(self):\n",
    "        \"\"\"\n",
    "        :return: A new MyScalar object with the value of the sine of this scalar\n",
    "        \"\"\"\n",
    "        return MyScalar(\n",
    "            torch.sin(torch.tensor(self.value)).item(),\n",
    "            torch.cos(torch.tensor(self.value)).item(),\n",
    "            self\n",
    "        )\n",
    "\n",
    "    def cos(self):\n",
    "        \"\"\"\n",
    "        :return: A new MyScalar object with the value of the cosine of this scalar\n",
    "        \"\"\"\n",
    "        return MyScalar(\n",
    "            torch.cos(torch.tensor(self.value)).item(),\n",
    "            -torch.sin(torch.tensor(self.value)).item(),\n",
    "            self\n",
    "        )\n",
    "\n",
    "    def __pow__(self, power, modulo=None):\n",
    "        \"\"\"\n",
    "        :param power: The power to raise this scalar to\n",
    "        :param modulo: The modulo to take the power  (Not used)\n",
    "        :return: A new MyScalar object with the value of this scalar raised to the power of `power`\n",
    "        \"\"\"\n",
    "        assert isinstance(power, (int, float))\n",
    "        return MyScalar(\n",
    "                torch.pow(torch.tensor(self.value), power).item(),\n",
    "                power * torch.pow(torch.tensor(self.value), power - 1).item(),\n",
    "                self\n",
    "            )\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        :param other: An int or float to add to this scalar\n",
    "        :return: A new MyScalar object with the value of this scalar plus `other`\n",
    "        \"\"\"\n",
    "        assert isinstance(other, (int, float))\n",
    "        return MyScalar(\n",
    "            self.value + other,\n",
    "            1.,\n",
    "            self\n",
    "        )\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        The reverse of __add__\n",
    "        \"\"\"\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        :param other: An int or float to multiply this scalar by\n",
    "        :return: A new MyScalar object with the value of this scalar multiplied by `other`\n",
    "        \"\"\"\n",
    "        assert isinstance(other, (int, float))\n",
    "        return MyScalar(\n",
    "                self.value * other,\n",
    "                other,\n",
    "                self\n",
    "            )\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"\n",
    "        The reverse of __mul__\n",
    "        \"\"\"\n",
    "        return self.__mul__(other)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test the new autograd system against PyTorch's autograd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a function $f(x) = \\log(\\sin(3x+4))$ and compute its gradient w.r.t $x$ at $x=2$.\n",
    "Just to verify the results, let's compute it by hand:\n",
    "$\\frac{\\mathrm{d}f}{\\mathrm{d}x} = \\frac{3\\cos(3x+4)}{\\sin(3x+4)} = 3\\cot(3x+4).$ Thus, $\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\Bigg\\vert_{x=2} = 3\\cot(10) \\approx 4.6271$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "4.62705288392008"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the gradient using the new autograd system\n",
    "a = MyScalar(2.)\n",
    "b = 3 * a\n",
    "c = b + 4\n",
    "d = MyScalar.sin(c)\n",
    "e = MyScalar.log(d)\n",
    "e.get_gradient()[id(a)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "4.6270527839660645"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with PyTorch's autograd system\n",
    "A = torch.tensor(2., requires_grad=True)\n",
    "B = 3 * A\n",
    "C = B + 4\n",
    "D = torch.sin(C)\n",
    "E = torch.log(D)\n",
    "E.backward()\n",
    "A.grad.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try with a more complicated function, $f(x) = \\exp(\\sin^{3/2}(2x^4 + 5))$ and compute it's gradient w.r.t $x$ at $x=1$.\n",
    "\n",
    "Just to verify the results, let's compute it by hand:\n",
    "$\\frac{\\mathrm{d}f}{\\mathrm{d}x} = 12 x^3 \\exp(\\sin^{3/2}(2x^4 + 5)) \\sqrt{\\sin(2x^4 + 5)} \\cos(2x^4 + 5).$ Thus, $\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\Bigg\\vert_{x=2} = 12 e^{\\sin^{3/2}(7)} \\sqrt{\\sin(7)} \\cos(7) \\approx 12.4895$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "12.489481868896883"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the gradient using the new autograd system\n",
    "a = MyScalar(1.)\n",
    "b = 2 * a ** 4\n",
    "c = b + 5\n",
    "d = MyScalar.sin(c) ** (3/2)\n",
    "e = MyScalar.exp(d)\n",
    "e.get_gradient()[id(a)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "12.489480972290039"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with PyTorch's autograd system\n",
    "A = torch.tensor(1., requires_grad=True)\n",
    "B = 2 * A ** 4\n",
    "C = B + 5\n",
    "D = torch.sin(C) ** (3/2)\n",
    "E = torch.exp(D)\n",
    "E.backward()\n",
    "A.grad.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
